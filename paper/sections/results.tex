%!TEX root = ../hr-paper.tex
\newpage
\section{Results} % (fold)
\label{sec:results}

This section will show the direct results of the intersection over union from the segmentation system. It will also show the results from the classification with Knn and the LVQ methods.

\subsection{Segmentation}

\begin{minipage}{\linewidth}
\centering
\captionof{table}{IoU Results} \label{tab:results:iou} 
\begin{tabular}{ c c c c c c c c c c}
\hline
\hline
\textit{0.1} & \textit{0.2} & \textit{0.3} & \textit{0.4} & \textit{0.5} & \textit{0.6} & \textit{0.7} & \textit{0.8} & \textit{0.9} & \textit{1}\\
0.988	&	0.973	&	0.948	&	0.905	&	0.841	&	0.769	&	0.704	&	0.631	&	0.532	&	0.117	\\
\hline
\end{tabular}\par
\flushleft
The results from the segmented characters with the labeled data from the dataset.
\bigskip
\bigskip
\end{minipage}

\subsection{Classification}

\subsubsection{Knn}

Because the Minkowsky distance measures for $p \geq 3$ preformed worse than the other distance measures they have been omitted in this section. Even-though the experiment was done for $K=1$ to $K=100$ we only show $K=1$ to $K=5$ because the classification rate steadily declines for an increasing $K$. The best result is made bold for all experiments ran.
\\\\
\begin{minipage}{\linewidth}
\flushleft
\captionof{table}{Knn base-set, Sobel} \label{tab:results:base:sobel} 
\begin{tabular}{r|ccccc}
\textbf{K} & \textbf{Cityblock} & \textbf{Euclidean} & \textbf{Euclidean(norm)} & \textbf{Mahalanobis} & \textbf{Cosine} \\
\hline
\hline
1 & 0.6887 & 0.6689 & 0.7219 & \textbf{0.7294} & 0.7234 \\
2 & 0.5900 & 0.5660 & 0.6308 & 0.6517 & 0.6289 \\
3 & 0.5517 & 0.5249 & 0.5984 & 0.6173 & 0.5964 \\
4 & 0.5217 & 0.4953 & 0.5698 & 0.5926 & 0.5674 \\
5 & 0.5018 & 0.4736 & 0.5551 & 0.5822 & 0.5528
% 6 & 0.4958 & 0.4635 & 0.5450 & 0.5754 & 0.5439 \\
% 7 & 0.4943 & 0.4598 & 0.5445 & 0.5761 & 0.5420 \\
% 8 & 0.4927 & 0.4567 & 0.5440 & 0.5734 & 0.5405 \\
% 9 & 0.4933 & 0.4552 & 0.5420 & 0.5730 & 0.5408 \\
% 10 & 0.4914 & 0.4525 & 0.5387 & 0.5726 & 0.5405 \\
% 11 & 0.4871 & 0.4514 & 0.5365 & 0.5701 & 0.5416 \\
% 12 & 0.4857 & 0.4486 & 0.5367 & 0.5680 & 0.5381 \\
% 13 & 0.4856 & 0.4486 & 0.5354 & 0.5674 & 0.5377 \\
% 14 & 0.4829 & 0.4473 & 0.5334 & 0.5658 & 0.5361 \\
% 15 & 0.4824 & 0.4455 & 0.5311 & 0.5639 & 0.5335 \\
% 16 & 0.4835 & 0.4433 & 0.5295 & 0.5618 & 0.5335 \\
% 17 & 0.4823 & 0.4421 & 0.5294 & 0.5608 & 0.5326 \\
% 18 & 0.4815 & 0.4412 & 0.5269 & 0.5586 & 0.5321 \\
% 19 & 0.4811 & 0.4407 & 0.5237 & 0.5568 & 0.5314 \\
% 20 & 0.4794 & 0.4400 & 0.5219 & 0.5563 & 0.5308
\end{tabular}\par
\bigskip
The classification rates of the feature extracted characters from the labeled base-set. Feature extraction was done with the Sobel filter and Knn with $K$ from 1 to 5.
\bigskip
\bigskip 
\end{minipage}

\begin{minipage}{\linewidth}
\flushleft
\captionof{table}{Knn base-set, Prewit} \label{tab:results:base:prewit} 
\begin{tabular}{r|ccccc}
\textbf{K} & \textbf{Cityblock} & \textbf{Euclidean} & \textbf{Euclidean(norm)} & \textbf{Mahalanobis} & \textbf{Cosine} \\
\hline
\hline
1 & 0.6509 & 0.7172 & 0.7219 & \textbf{0.7432} & 0.6241 \\
2 & 0.5410 & 0.6242 & 0.6308 & 0.6647 & 0.5057 \\
3 & 0.4996 & 0.5964 & 0.5984 & 0.6349 & 0.4616 \\
4 & 0.4649 & 0.5685 & 0.5698 & 0.6146 & 0.4190 \\
5 & 0.4458 & 0.5534 & 0.5551 & 0.6037 & 0.3906
% 6 & 0.4362 & 0.5470 & 0.5450 & 0.5978 & 0.3748 \\
% 7 & 0.4297 & 0.5447 & 0.5445 & 0.5955 & 0.3698 \\
% 8 & 0.4265 & 0.5425 & 0.5440 & 0.5904 & 0.3674 \\
% 9 & 0.4264 & 0.5413 & 0.5420 & 0.5896 & 0.3656 \\
% 10 & 0.4260 & 0.5389 & 0.5387 & 0.5902 & 0.3646 \\
% 11 & 0.4255 & 0.5375 & 0.5365 & 0.5886 & 0.3600 \\
% 12 & 0.4247 & 0.5373 & 0.5367 & 0.5852 & 0.3599 \\
% 13 & 0.4252 & 0.5372 & 0.5354 & 0.5812 & 0.3593 \\
% 14 & 0.4257 & 0.5348 & 0.5334 & 0.5816 & 0.3598 \\
% 15 & 0.4259 & 0.5326 & 0.5311 & 0.5807 & 0.3587 \\
% 16 & 0.4257 & 0.5293 & 0.5295 & 0.5797 & 0.3594 \\
% 17 & 0.4253 & 0.5262 & 0.5294 & 0.5771 & 0.3583 \\
% 18 & 0.4282 & 0.5252 & 0.5269 & 0.5761 & 0.3580 \\
% 19 & 0.4274 & 0.5235 & 0.5237 & 0.5741 & 0.3561 \\
% 20 & 0.4272 & 0.5219 & 0.5219 & 0.5719 & 0.3561 
\end{tabular}\par
\bigskip
The classification rates of the feature extracted characters from the labeled base-set. Feature extraction was done with the Prewit filter and Knn with $K$ from 1 to 5.
\bigskip
\bigskip
\end{minipage}



\begin{minipage}{\linewidth}
\flushleft
\captionof{table}{Knn segmented-set, Sobel} \label{tab:results:seg:sobel} 
\begin{tabular}{r|ccccc}
\textbf{K} & \textbf{Cityblock} & \textbf{Euclidean} & \textbf{Euclidean(norm)} & \textbf{Mahalanobis} & \textbf{Cosine} \\
\hline
\hline
1 & 0.8054 & \textbf{0.8305} & 0.8275 & 0.8179 & 0.7940 \\
2 & 0.6865 & 0.7267 & 0.7252 & 0.7149 & 0.6676 \\
3 & 0.6274 & 0.6789 & 0.6720 & 0.6621 & 0.6091 \\
4 & 0.5911 & 0.6438 & 0.6384 & 0.6322 & 0.5732 \\
5 & 0.5748 & 0.6348 & 0.6308 & 0.6159 & 0.5557
% 6 & 0.5672 & 0.6278 & 0.6271 & 0.6106 & 0.5503 \\
% 7 & 0.5693 & 0.6295 & 0.6267 & 0.6093 & 0.5456 \\
% 8 & 0.5666 & 0.6312 & 0.6269 & 0.6067 & 0.5449 \\
% 9 & 0.5665 & 0.6313 & 0.6245 & 0.6056 & 0.5438 \\
% 10 & 0.5671 & 0.6309 & 0.6235 & 0.6018 & 0.5429 \\
% 11 & 0.5644 & 0.6275 & 0.6204 & 0.5991 & 0.5441 \\
% 12 & 0.5601 & 0.6224 & 0.6193 & 0.5964 & 0.5403 \\
% 13 & 0.5601 & 0.6199 & 0.6145 & 0.5960 & 0.5390 \\
% 14 & 0.5594 & 0.6165 & 0.6127 & 0.5960 & 0.5374 \\
% 15 & 0.5593 & 0.6164 & 0.6090 & 0.5929 & 0.5390 \\
% 16 & 0.5580 & 0.6118 & 0.6061 & 0.5929 & 0.5363 \\
% 17 & 0.5579 & 0.6086 & 0.6034 & 0.5908 & 0.5358 \\
% 18 & 0.5557 & 0.6059 & 0.6012 & 0.5897 & 0.5351 \\
% 19 & 0.5545 & 0.6053 & 0.5978 & 0.5880 & 0.5344 \\
% 20 & 0.5552 & 0.6043 & 0.5977 & 0.5861 & 0.5312
\end{tabular}\par
\bigskip
The classification rates with the segmented and feature extracted characters. Feature extraction was done with the Sobel filter and Knn with $K$ from 1 to 5.
\bigskip
\bigskip
\end{minipage}


%knn prewit seg

\begin{minipage}{\linewidth}
\flushleft
\captionof{table}{Knn segmented-set, Prewit} \label{tab:results:seg:prewit} 
\begin{tabular}{r|ccccc}
\textbf{K} & \textbf{Cityblock} & \textbf{Euclidean} & \textbf{Euclidean(norm)} & \textbf{Mahalanobis} & \textbf{Cosine} \\
\hline
\hline
1 & 0.7701 & 0.7429 & 0.8229 & \textbf{0.8254} & 0.7456 \\
2 & 0.6281 & 0.5903 & 0.7129 & 0.7234 & 0.5925 \\
3 & 0.5659 & 0.5149 & 0.6626 & 0.6714 & 0.5223 \\
4 & 0.5209 & 0.4623 & 0.6303 & 0.6442 & 0.4702 \\
5 & 0.4996 & 0.4319 & 0.6178 & 0.6343 & 0.4396
% 6 & 0.4893 & 0.4223 & 0.6076 & 0.6281 & 0.4257 \\
% 7 & 0.4877 & 0.4167 & 0.6107 & 0.6281 & 0.4222 \\
% 8 & 0.4857 & 0.4106 & 0.6089 & 0.6249 & 0.4146 \\
% 9 & 0.4846 & 0.4066 & 0.6060 & 0.6234 & 0.4126 \\
% 10 & 0.4823 & 0.4062 & 0.6009 & 0.6204 & 0.4137 \\
% 11 & 0.4848 & 0.4028 & 0.5993 & 0.6202 & 0.4140 \\
% 12 & 0.4846 & 0.4022 & 0.5974 & 0.6178 & 0.4121 \\
% 13 & 0.4839 & 0.4023 & 0.5961 & 0.6162 & 0.4107 \\
% 14 & 0.4826 & 0.4002 & 0.5941 & 0.6128 & 0.4100 \\
% 15 & 0.4815 & 0.3991 & 0.5923 & 0.6121 & 0.4077 \\
% 16 & 0.4819 & 0.3964 & 0.5906 & 0.6096 & 0.4076 \\
% 17 & 0.4789 & 0.3947 & 0.5875 & 0.6100 & 0.4067 \\
% 18 & 0.4792 & 0.3943 & 0.5855 & 0.6078 & 0.4068 \\
% 19 & 0.4799 & 0.3935 & 0.5829 & 0.6067 & 0.4067 \\
% 20 & 0.4800 & 0.3916 & 0.5810 & 0.6047 & 0.4069 
\end{tabular}\par
\bigskip
The classification rates with the segmented and feature extracted characters. Feature extraction was done with the Prewit filter and Knn with $K$ from 1 to 5.
\bigskip
\bigskip
\end{minipage}

\subsubsection{LVQ}

\begin{minipage}{\linewidth}
\centering
\captionof{table}{LVQ parameter sweep} \label{tab:results:lvq:param-sweep} 
\begin{tabular}{r|cccc}
\textbf{Normalization} & \textbf{GLVQ}                & \textbf{GRLVQ}               & \textbf{GMLVQ}               & \textbf{LGMLVQ}                    \\
\hline
\hline
NONE          & 0.0236 (0.0236) & 0.0176 (0.0179) & 0.0192 (0.0197) & 0.0270 (0.0261)\\
NORM          & 0.0307 (0.0317) & 0.0265 (0.0265) & 0.0302 (0.0301) & 0.0379 (0.0393)\\
VAR           & 0.5208 (0.5791) & 0.4487 (0.5010) & 0.5175 (0.5755) & \textbf{0.6499 (0.7826)}\\
\end{tabular}\par
\flushleft
Different versions of the LVQ algorithm on the segmented dataset, feature extracted with Sobel and 2000 iteration.
\bigskip
\end{minipage}

\noindent Because the LGMLVQ algorithm performed best during the parameter sweep, it is used to run on all datasets, the results are shown below in table \ref{tab:results:LGMLVQ:var:8000}.
\\\\\
\begin{minipage}{\linewidth}
\centering
\captionof{table}{LGMLVQ results on all datasets} \label{tab:results:LGMLVQ:var:8000} 
\begin{tabular}{cccc}
\hline
\hline
\textit{Sobel Seg} & \textit{Sobel Base} & \textit{Prewit Seg}      & \textit{Prewit Base} \\
0.6755 (0.7535)    & 0.6755 (0.7453)     & \textbf{0.6987 (0.7923)} & 0.6725 (0.78)\\
\hline
\end{tabular}\par
\flushleft
LGMLVQ results on the two different datasets feature extracted with the Sobel and Prewit filters. The max iteration was 8000. The test error is given and the train error is given in parenthesis. The datasets were preprocessed with the variance between one and zero.
\end{minipage}




%2comps





% - image to big due to line noise
% image to big due to other noise
% correct small
% 2 in one
% - 1.5

% section results (end)
